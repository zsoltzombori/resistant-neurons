{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import itertools\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import glob\n",
    "\n",
    "#from keras import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.mixture\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../neuron_logs/train_data/output_tmp_20191217-185451.json', '../neuron_logs/train_data/output_tmp_20191217-185454.json', '../neuron_logs/train_data/output_tmp_20191217-185500.json', '../neuron_logs/train_data/output_tmp_20191217-185505.json', '../neuron_logs/train_data/output_tmp_20191217-185510.json', '../neuron_logs/train_data/output_tmp_20191217-185515.json', '../neuron_logs/train_data/output_tmp_20191217-185520.json', '../neuron_logs/train_data/output_tmp_20191217-185525.json', '../neuron_logs/train_data/output_tmp_20191217-185531.json', '../neuron_logs/train_data/output_tmp_20191217-192822.json', '../neuron_logs/train_data/output_tmp_20191217-192843.json', '../neuron_logs/train_data/output_tmp_20191217-192850.json', '../neuron_logs/train_data/output_tmp_20191217-192907.json', '../neuron_logs/train_data/output_tmp_20191217-193031.json', '../neuron_logs/train_data/output_tmp_20191217-193104.json', '../neuron_logs/train_data/output_tmp_20191217-193112.json', '../neuron_logs/train_data/output_tmp_20191217-193115.json', '../neuron_logs/train_data/output_tmp_20191217-193204.json', '../neuron_logs/train_data/output_tmp_20191217-200153.json', '../neuron_logs/train_data/output_tmp_20191217-200246.json', '../neuron_logs/train_data/output_tmp_20191217-200305.json', '../neuron_logs/train_data/output_tmp_20191217-200308.json', '../neuron_logs/train_data/output_tmp_20191217-200648.json', '../neuron_logs/train_data/output_tmp_20191217-200713.json', '../neuron_logs/train_data/output_tmp_20191217-200717.json', '../neuron_logs/train_data/output_tmp_20191217-200750.json']\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "neuron_data = {}\n",
    "\n",
    "no_of_testfiles = 5\n",
    "\n",
    "files = sorted(glob.glob('../neuron_logs/train_data/*.json'))\n",
    "print(files)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_statistics(activations, labels, debug=False):\n",
    "    sorted_data = []\n",
    "    for i in range(10): #hardcoded MOFO\n",
    "        sorted_data += [[]]\n",
    "    for i, a in zip(labels, activations):\n",
    "        sorted_data[i] += [a]\n",
    "    if debug:\n",
    "        return(sorted_data)\n",
    "    statistics = []\n",
    "    for ar in sorted_data:\n",
    "        curr_stats = stats.describe(ar)\n",
    "        statistics += [curr_stats.mean, curr_stats.variance, curr_stats.skewness, curr_stats.kurtosis, curr_stats.minmax[0],\n",
    "                       curr_stats.minmax[1], curr_stats.nobs]\n",
    "        #print(statistics)\n",
    "    return(statistics)\n",
    "\n",
    "def weight_statistics(weights):\n",
    "    stats = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../neuron_logs/train_data/output_tmp_20191217-185451.json\n"
     ]
    }
   ],
   "source": [
    "filename = files[0]\n",
    "with open(filename, 'r') as f:\n",
    "    print(filename)\n",
    "    neuron_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11922008618712425"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(neuron_data['5']['3 20']['input_weights'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
       "                means_init=None, n_components=5, n_init=1, precisions_init=None,\n",
       "                random_state=None, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "                verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = np.array(neuron_data['5']['3 20']['input_weights'])\n",
    "gmm = sklearn.mixture.GaussianMixture(n_components = 5, covariance_type = 'diag')\n",
    "gmm.fit(ex.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15766252, 0.20535913, 0.26048986, 0.20551476, 0.17097373])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 4., 2., 7., 4., 4., 4., 4., 5., 8., 6., 5., 6., 6., 3., 7., 2.,\n",
       "        7., 6., 7.]),\n",
       " array([-0.1728957 , -0.15521967, -0.13754364, -0.11986761, -0.10219158,\n",
       "        -0.08451555, -0.06683952, -0.04916349, -0.03148746, -0.01381143,\n",
       "         0.0038646 ,  0.02154063,  0.03921666,  0.05689269,  0.07456872,\n",
       "         0.09224475,  0.10992078,  0.12759681,  0.14527284,  0.16294887,\n",
       "         0.1806249 ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ex, bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../neuron_logs/train_data/output_tmp_20191217-185515.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-185520.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-185525.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-185531.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-192822.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-192843.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-192850.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-192907.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-193031.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-193104.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-193112.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-193115.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-193204.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-200153.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-200246.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-200305.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-200308.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-200648.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-200713.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-200717.json\n",
      "../neuron_logs/train_data/output_tmp_20191217-200750.json\n"
     ]
    }
   ],
   "source": [
    "features, labels = [], []\n",
    "target = 'usefulness_loss'\n",
    "\n",
    "for i, filename in enumerate(files[no_of_testfiles:]):\n",
    "    with open(filename, 'r') as f:\n",
    "        print(filename)\n",
    "        neuron_data = json.load(f)\n",
    "    for e in neuron_data.keys():\n",
    "        for neuron in neuron_data[e]:\n",
    "            if ' ' not in neuron:\n",
    "                continue\n",
    "            current_data = neuron_data[e][neuron]\n",
    "            important_features = []\n",
    "            important_features = current_data['activations']\n",
    "            # important_features += reduce_to_statistics(current_data['activations'], neuron_data[e]['original_labels'])\n",
    "            important_features += [current_data['depth']]\n",
    "            important_features += [current_data['inverse_depth']]\n",
    "            important_features += [current_data['width']]\n",
    "            important_features += [np.percentile(current_data['input_weights'], p) for p in range(0, 101, 5)]\n",
    "            important_features += [np.percentile(current_data['output_weights'], p) for p in range(0, 101, 5)]\n",
    "            gmm_input = sklearn.mixture.GaussianMixture(n_components = 5, covariance_type = 'diag').fit(np.array(current_data['input_weights']).reshape(-1, 1))\n",
    "            gmm_output = sklearn.mixture.GaussianMixture(n_components = 5, covariance_type = 'diag').fit(np.array(current_data['output_weights']).reshape(-1, 1))\n",
    "            important_features += list(gmm_input.means_.flatten()) + list(gmm_input.covariances_.flatten()) + list(gmm_input.weights_)\n",
    "            important_features += list(gmm_output.means_.flatten()) + list(gmm_output.covariances_.flatten()) + list(gmm_output.weights_)\n",
    "            important_features += sorted(current_data['input_weights'])[:10]\n",
    "            important_features += sorted(current_data['input_weights'])[-10:]\n",
    "            important_features += sorted(current_data['output_weights'])[:10]\n",
    "            important_features += sorted(current_data['output_weights'])[-10:]\n",
    "            important_features += [current_data['reg_loss_in_layer']]\n",
    "            important_features += [e]\n",
    "            features += [important_features]\n",
    "            #labels += [current_data[target]]\n",
    "            labels += [current_data[target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, target = 'usefulness_loss', shuffle = True):\n",
    "    \n",
    "    features, labels = [], []\n",
    "    with open(filename, 'r') as f:\n",
    "        neuron_data = json.load(f)\n",
    "        for e in neuron_data.keys():\n",
    "            for neuron in neuron_data[e]:\n",
    "                if ' ' not in neuron:\n",
    "                    continue\n",
    "                current_data = neuron_data[e][neuron]\n",
    "                important_features = []\n",
    "                important_features = current_data['activations']\n",
    "                #important_features += reduce_to_statistics(current_data['activations'], neuron_data[e]['original_labels'])\n",
    "                important_features += [current_data['depth']]\n",
    "                important_features += [current_data['inverse_depth']]\n",
    "                important_features += [current_data['width']]\n",
    "                important_features += [np.percentile(current_data['input_weights'], p) for p in range(0, 101, 5)]\n",
    "                important_features += [np.percentile(current_data['output_weights'], p) for p in range(0, 101, 5)]\n",
    "                gmm_input = sklearn.mixture.GaussianMixture(n_components = 5, covariance_type = 'diag').fit(np.array(current_data['input_weights']).reshape(-1, 1))\n",
    "                gmm_output = sklearn.mixture.GaussianMixture(n_components = 5, covariance_type = 'diag').fit(np.array(current_data['output_weights']).reshape(-1, 1))\n",
    "                important_features += list(gmm_input.means_.flatten()) + list(gmm_input.covariances_.flatten()) + list(gmm_input.weights_)\n",
    "                important_features += list(gmm_output.means_.flatten()) + list(gmm_output.covariances_.flatten()) + list(gmm_output.weights_)\n",
    "                important_features += sorted(current_data['input_weights'])[:10]\n",
    "                important_features += sorted(current_data['input_weights'])[-10:]\n",
    "                important_features += sorted(current_data['output_weights'])[:10]\n",
    "                important_features += sorted(current_data['output_weights'])[-10:]\n",
    "                important_features += [current_data['reg_loss_in_layer']]\n",
    "                important_features += [e]\n",
    "                features += [important_features]\n",
    "                #labels += [current_data[target]]\n",
    "                labels += [current_data[target]]\n",
    "    \n",
    "    if shuffle:\n",
    "        shuffled = sklearn.utils.shuffle(np.concatenate([np.array(features), np.array(labels).reshape(-1, 1)], axis=1))\n",
    "        features, labels = shuffled[:, :-1], shuffled[:, -1]\n",
    "    \n",
    "    return(np.array(features, dtype=np.float32), np.array(labels, dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115500, 1117) (115500,)\n"
     ]
    }
   ],
   "source": [
    "features = np.array(features, dtype=np.float32)\n",
    "labels = np.array(labels, dtype=np.float32)\n",
    "print(features.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(features, labels, test_size=3/len(files[no_of_testfiles:]), random_state=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "X_test = X_valid.astype(np.float32)\n",
    "y_test = y_valid.astype(np.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "validscaler = StandardScaler()\n",
    "scaler.fit(X_valid)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99000, 1117) (16500, 1117) (99000,) (16500,)\n",
      "MSE on unknown network: 0.000585982168558985\n"
     ]
    }
   ],
   "source": [
    "print(f\"{X_train.shape} {X_valid.shape} {y_train.shape} {y_valid.shape}\")\n",
    "reg = linear_model.LinearRegression(n_jobs = -1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_valid)\n",
    "print(f\"MSE on unknown network: {mean_squared_error(y_pred, y_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = extract_data(files[0])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on unknown network: 0.0003217359771952033\n",
      "[1.0474983  1.011138   1.0319812  0.9955828  0.9997907  1.0075389\n",
      " 1.0218141  1.0002183  1.0193996  1.002361   0.9928949  1.0002232\n",
      " 1.         1.0247984  1.0003877  1.005197   1.0135914  1.0068516\n",
      " 1.0046741  0.99984616 1.0219252  1.0693161  0.99156827 0.9781961\n",
      " 1.0069487  1.0328195  0.9830215  0.9926676  1.0413073  1.0045397\n",
      " 1.006151   1.004194   0.98487157 1.0000035  1.0000001  1.0387605\n",
      " 0.99886614 1.0069089  1.0243331  0.9896616  0.989046   1.001485\n",
      " 0.99650395 1.0012414  1.0014613  0.99035543 1.0276892  1.0032827\n",
      " 1.         0.9839617 ]\n",
      "[1.0164326  1.0165007  1.0134516  1.0074266  1.002527   1.0078343\n",
      " 1.0109715  0.999795   1.0202118  1.0059644  1.0129075  1.0067809\n",
      " 0.9977047  1.0146434  0.9975362  1.0100787  1.0062212  1.0004852\n",
      " 1.0154258  0.9983441  1.0130594  1.021353   1.0048745  1.0103987\n",
      " 1.0076865  1.01242    1.0231357  1.0290506  1.0207671  1.0210624\n",
      " 1.0205742  1.0060163  1.0174294  0.99794173 0.9971179  1.0298558\n",
      " 1.0234599  1.0148324  1.0238374  0.99733    1.0059441  0.992951\n",
      " 1.0103681  1.0076624  1.0146179  1.0125753  1.0306458  1.0200294\n",
      " 1.0012863  1.0227404 ]\n",
      "[-0.0310657   0.00536275 -0.01852965  0.0118438   0.00273627  0.0002954\n",
      " -0.01084256 -0.00042325  0.00081217  0.00360334  0.02001262  0.0065577\n",
      " -0.00229532 -0.01015496 -0.00285149  0.00488162 -0.00737023 -0.00636637\n",
      "  0.01075172 -0.00150204 -0.00886583 -0.04796314  0.0133062   0.03220266\n",
      "  0.00073779 -0.02039945  0.04011416  0.03638297 -0.02054024  0.01652265\n",
      "  0.01442325  0.00182223  0.03255779 -0.00206172 -0.00288224 -0.0089047\n",
      "  0.02459377  0.00792348 -0.00049567  0.00766844  0.01689816 -0.00853401\n",
      "  0.01386416  0.00642097  0.01315665  0.02221984  0.00295663  0.01674676\n",
      "  0.00128627  0.03877866]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levai/miniconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=5.13738e-09): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Ridge(alpha = 0.5)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"MSE on unknown network: {mean_squared_error(y_pred, y_test)}\")\n",
    "print(y_test[:50])\n",
    "print(y_pred[:50])\n",
    "print(y_pred[:50] - y_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00039298655\n",
      "MSE on unknown network: 0.0003929865488316864\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(mean_squared_error(y_pred, y_test))\n",
    "print(f\"MSE on unknown network: {mean_squared_error(y_pred, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1218 10:34:37.542173 140644729902912 deprecation_wrapper.py:119] From /home/levai/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1218 10:34:37.585296 140644729902912 deprecation_wrapper.py:119] From /home/levai/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1218 10:34:37.604822 140644729902912 deprecation_wrapper.py:119] From /home/levai/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1218 10:34:37.632853 140644729902912 deprecation_wrapper.py:119] From /home/levai/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1218 10:34:37.641288 140644729902912 deprecation.py:506] From /home/levai/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1218 10:34:37.778758 140644729902912 deprecation_wrapper.py:119] From /home/levai/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1218 10:34:38.018184 140644729902912 deprecation_wrapper.py:119] From /home/levai/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99000 samples, validate on 16500 samples\n",
      "Epoch 1/10\n",
      "99000/99000 [==============================] - 39s 391us/step - loss: 0.0628 - mean_squared_error: 0.0153 - val_loss: 0.0113 - val_mean_squared_error: 3.3173e-04\n",
      "Epoch 2/10\n",
      "99000/99000 [==============================] - 39s 391us/step - loss: 0.0118 - mean_squared_error: 3.4319e-04 - val_loss: 0.0113 - val_mean_squared_error: 3.0542e-04\n",
      "Epoch 3/10\n",
      "99000/99000 [==============================] - 36s 366us/step - loss: 0.0115 - mean_squared_error: 3.3678e-04 - val_loss: 0.0111 - val_mean_squared_error: 3.0300e-04\n",
      "Epoch 4/10\n",
      "99000/99000 [==============================] - 38s 380us/step - loss: 0.0115 - mean_squared_error: 3.3886e-04 - val_loss: 0.0114 - val_mean_squared_error: 3.4547e-04\n",
      "Epoch 5/10\n",
      "99000/99000 [==============================] - 38s 386us/step - loss: 0.0114 - mean_squared_error: 3.3365e-04 - val_loss: 0.0113 - val_mean_squared_error: 3.2435e-04\n",
      "Epoch 6/10\n",
      "99000/99000 [==============================] - 37s 377us/step - loss: 0.0114 - mean_squared_error: 3.3194e-04 - val_loss: 0.0113 - val_mean_squared_error: 3.3877e-04\n",
      "Epoch 7/10\n",
      "99000/99000 [==============================] - 38s 381us/step - loss: 0.0114 - mean_squared_error: 3.3326e-04 - val_loss: 0.0112 - val_mean_squared_error: 3.0295e-04\n",
      "Epoch 8/10\n",
      "99000/99000 [==============================] - 41s 413us/step - loss: 0.0114 - mean_squared_error: 3.2985e-04 - val_loss: 0.0118 - val_mean_squared_error: 3.6941e-04\n",
      "Epoch 9/10\n",
      "99000/99000 [==============================] - 41s 415us/step - loss: 0.0114 - mean_squared_error: 3.3130e-04 - val_loss: 0.0111 - val_mean_squared_error: 3.0534e-04\n",
      "Epoch 10/10\n",
      "99000/99000 [==============================] - 39s 389us/step - loss: 0.0114 - mean_squared_error: 3.3051e-04 - val_loss: 0.0113 - val_mean_squared_error: 3.3810e-04\n",
      "MSE on unknown network: 0.0003266784187871963\n"
     ]
    }
   ],
   "source": [
    "reg = Sequential()\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001, min_delta=0.0001)\n",
    "WIDTH=100\n",
    "reg.add(Dense(WIDTH, kernel_initializer = 'uniform', activation = 'relu', input_dim=X_train.shape[1]))\n",
    "reg.add(Dropout(rate=0.25))\n",
    "reg.add(Dense(WIDTH, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "reg.add(Dropout(rate=0.25))\n",
    "reg.add(Dense(WIDTH//2, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "reg.add(Dropout(rate=0.25))\n",
    "reg.add(Dense(1, kernel_initializer = 'uniform', activation = 'linear'))\n",
    "reg.compile(optimizer= 'adam', loss = keras.losses.mean_absolute_error, metrics = [metrics.mse])\n",
    "reg.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data=[X_valid, y_valid], shuffle=True, callbacks=[reduce_lr])\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"MSE on unknown network: {mean_squared_error(y_pred, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0100465],\n",
       "       [1.0113871],\n",
       "       [1.0055512],\n",
       "       ...,\n",
       "       [1.0031708],\n",
       "       [1.0000203],\n",
       "       [1.0031708]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on unknown network: 0.0003355958353024621\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(f\"MSE on unknown network: {mean_squared_error(y_pred, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.026189   0.99833494 0.98953813 1.004194   1.0000085\n",
      " 1.0025307  1.0318785  1.011138   1.0169818  0.99028563 0.95293224\n",
      " 0.9992968  1.0108587  1.0251158  1.0007721  1.0000001  1.0000689\n",
      " 1.0310318  1.0046772  1.0122964  1.0277824  1.0205061  1.0098892\n",
      " 1.0400382  0.9989925  0.99979854 1.0172278  1.0151457  1.0128899\n",
      " 1.0332916  1.0102363  1.010086   0.99888694 1.0043974  1.001846\n",
      " 1.011782   0.99092877 1.0147691  0.997837   1.0016624  1.0092485\n",
      " 1.0037268  1.0106306  1.0142219  0.9908123  1.0100315  1.\n",
      " 1.0012201  0.9866134 ]\n",
      "[1.0002685  1.04498898 0.99765883 0.99771341 0.99991665 1.00356332\n",
      " 1.00539912 1.00308009 1.01196353 1.00992168 1.00000949 1.00031412\n",
      " 0.99914721 1.00772486 1.01781344 1.01218524 1.00206344 0.99763775\n",
      " 1.02413841 0.99915682 1.01285025 1.03831145 1.00880171 0.98648252\n",
      " 1.02613372 0.99598987 1.02745027 1.02122883 1.00842326 1.0192936\n",
      " 1.00675082 1.01075832 0.99481479 1.01561954 1.00248897 1.00247336\n",
      " 0.98448388 1.00083061 1.00586702 1.01369257 1.01299659 1.01940973\n",
      " 0.99958127 1.00030948 1.00664271 1.00411768 0.99952937 0.9966848\n",
      " 0.99774406 1.00303436]\n",
      "[ 0.0002685   0.01880001 -0.00067611  0.00817527 -0.00427737  0.00355486\n",
      "  0.00286842 -0.02879839  0.00082556 -0.00706016  0.00972385  0.04738188\n",
      " -0.00014958 -0.00313379 -0.00730241  0.01141312  0.00206333 -0.00243116\n",
      " -0.00689344 -0.00552036  0.00055382  0.01052901 -0.01170443 -0.02340673\n",
      " -0.01390451 -0.00300263  0.02765173  0.00400106 -0.0067224   0.00640374\n",
      " -0.02654076  0.00052206 -0.01527127  0.0167326  -0.00190843  0.0006274\n",
      " -0.02729817  0.00990184 -0.00890206  0.01585556  0.01133421  0.01016123\n",
      " -0.00414557 -0.01032112 -0.00757919  0.01330537 -0.01050209 -0.0033152\n",
      " -0.00347604  0.01642097]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:50])\n",
    "print(y_pred[:50])\n",
    "print(y_pred[:50] - y_test[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import skorch\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch import NeuralNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=1000,\n",
    "            nonlin=F.relu,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(RegressorModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.dense0 = nn.Linear(X_train.shape[1], num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dense1 = nn.Linear(num_units, num_units//2)\n",
    "        self.dense2 = nn.Linear(num_units//2, num_units//2)\n",
    "        self.dense3 = nn.Linear(num_units//2, num_units//2)\n",
    "        self.denselast = nn.Linear(num_units//2, 100)\n",
    "        self.output = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = F.relu(self.dense2(X))\n",
    "        X = F.relu(self.dense3(X))\n",
    "        X = F.relu(self.denselast(X))\n",
    "        X = self.output(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "\n",
    "lrscheduler = LRScheduler(monitor='train_loss',\n",
    "            policy='ReduceLROnPlateau', # step_size=15, gamma=0.5)\n",
    "            mode='min', factor=0.2, patience=10, verbose=True, \n",
    "            threshold=0.00001, threshold_mode='rel', \n",
    "            cooldown=0, min_lr=0, eps=1e-08)\n",
    "\n",
    "from skorch.callbacks import Checkpoint\n",
    "\n",
    "checkpoint = Checkpoint(\n",
    "    f_params='best_model.pt', monitor='valid_loss_best')\n",
    "\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor='train_loss',\n",
    "    patience=20,\n",
    "    threshold=0.0001,\n",
    "    threshold_mode='abs',\n",
    "    lower_is_better=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    RegressorModule,\n",
    "    max_epochs=200,\n",
    "    lr=0.1,\n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__momentum=0.9,\n",
    "    # callbacks=[skorch.callbacks.LRScheduler(skorch.callbacks.WarmRestartLR)],\n",
    "    callbacks = [lrscheduler, checkpoint, earlystopping],\n",
    "    warm_start=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.0987\u001b[0m        \u001b[32m0.0307\u001b[0m     +  3.3803\n",
      "      2        \u001b[36m0.0281\u001b[0m        \u001b[32m0.0257\u001b[0m     +  2.9467\n",
      "      3        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0214\u001b[0m     +  2.8305\n",
      "      4        \u001b[36m0.0218\u001b[0m        \u001b[32m0.0208\u001b[0m     +  2.9731\n",
      "      5        \u001b[36m0.0198\u001b[0m        0.0221        3.0816\n",
      "      6        \u001b[36m0.0186\u001b[0m        0.0214        3.0854\n",
      "      7        \u001b[36m0.0173\u001b[0m        0.0212        3.0655\n",
      "      8        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0205\u001b[0m     +  3.0977\n",
      "      9        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0189\u001b[0m     +  2.9241\n",
      "     10        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0172\u001b[0m     +  2.9117\n",
      "     11        \u001b[36m0.0127\u001b[0m        0.0177        2.9575\n",
      "     12        \u001b[36m0.0116\u001b[0m        0.0173        3.6116\n",
      "     13        0.0119        0.0193        3.5493\n",
      "     14        0.0119        0.0213        3.3963\n",
      "     15        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0164\u001b[0m     +  3.0056\n",
      "     16        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0112\u001b[0m     +  3.2633\n",
      "     17        \u001b[36m0.0082\u001b[0m        \u001b[32m0.0097\u001b[0m     +  5.1497\n",
      "     18        \u001b[36m0.0074\u001b[0m        \u001b[32m0.0073\u001b[0m     +  4.3055\n",
      "     19        \u001b[36m0.0064\u001b[0m        0.0075        3.5835\n",
      "     20        \u001b[36m0.0064\u001b[0m        \u001b[32m0.0072\u001b[0m     +  3.5225\n",
      "     21        \u001b[36m0.0060\u001b[0m        0.0073        2.9245\n",
      "     22        \u001b[36m0.0056\u001b[0m        \u001b[32m0.0067\u001b[0m     +  3.0195\n",
      "     23        \u001b[36m0.0054\u001b[0m        0.0073        2.9181\n",
      "     24        \u001b[36m0.0050\u001b[0m        \u001b[32m0.0058\u001b[0m     +  2.9408\n",
      "     25        \u001b[36m0.0048\u001b[0m        \u001b[32m0.0058\u001b[0m     +  2.8708\n",
      "     26        \u001b[36m0.0046\u001b[0m        \u001b[32m0.0053\u001b[0m     +  3.0866\n",
      "     27        \u001b[36m0.0042\u001b[0m        \u001b[32m0.0049\u001b[0m     +  3.2154\n",
      "     28        \u001b[36m0.0041\u001b[0m        0.0053        3.0335\n",
      "     29        0.0041        0.0050        3.4558\n",
      "     30        \u001b[36m0.0040\u001b[0m        \u001b[32m0.0047\u001b[0m     +  3.4476\n",
      "     31        \u001b[36m0.0040\u001b[0m        0.0048        3.2995\n",
      "     32        0.0040        \u001b[32m0.0047\u001b[0m     +  3.3942\n",
      "     33        0.0041        \u001b[32m0.0045\u001b[0m     +  3.1997\n",
      "     34        \u001b[36m0.0039\u001b[0m        \u001b[32m0.0040\u001b[0m     +  4.2408\n",
      "     35        \u001b[36m0.0036\u001b[0m        \u001b[32m0.0039\u001b[0m     +  4.7193\n",
      "     36        \u001b[36m0.0034\u001b[0m        0.0041        4.8334\n",
      "     37        \u001b[36m0.0031\u001b[0m        0.0045        2.9520\n",
      "     38        \u001b[36m0.0029\u001b[0m        0.0041        2.9916\n",
      "     39        \u001b[36m0.0027\u001b[0m        0.0047        3.1568\n",
      "     40        0.0027        0.0053        3.3130\n",
      "     41        \u001b[36m0.0026\u001b[0m        0.0050        3.4069\n",
      "     42        \u001b[36m0.0025\u001b[0m        0.0049        3.4604\n",
      "     43        0.0025        0.0051        2.8380\n",
      "     44        0.0025        0.0055        3.1692\n",
      "     45        0.0025        0.0058        3.0551\n",
      "     46        0.0027        0.0058        3.0813\n",
      "     47        0.0027        0.0060        3.1145\n",
      "     48        0.0028        0.0064        3.1200\n",
      "     49        0.0029        0.0063        2.9759\n",
      "     50        0.0031        0.0064        3.4710\n",
      "     51        0.0031        0.0062        3.3585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=RegressorModule(\n",
       "    (dense0): Linear(in_features=2075, out_features=1000, bias=True)\n",
       "    (dense1): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (dense2): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (dense3): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (denselast): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (output): Linear(in_features=100, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on unknown network: 0.03272349759936333\n"
     ]
    }
   ],
   "source": [
    "y_pred = net.predict(X_test)\n",
    "print(f\"MSE on unknown network: {mean_squared_error(y_pred, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-78e67bf05887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#valid_acc = [x['valid_acc'] for x in net.history]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-78e67bf05887>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#valid_acc = [x['valid_acc'] for x in net.history]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_loss'"
     ]
    }
   ],
   "source": [
    "#valid_acc = [x['valid_acc'] for x in net.history]\n",
    "train_loss = [x['train_loss'] for x in net.history]\n",
    "valid_loss = [x['valid_loss'] for x in net.history]\n",
    "fig, ax1 = plt.subplots(figsize = (12, 8), facecolor = 'w')\n",
    "\n",
    "# ax1.plot(range(len(valid_acc)), valid_acc, color='firebrick', label = 'neural net validation acc')\n",
    "\n",
    "\n",
    "ax1.plot(range(len(valid_loss)), [reg.score(X_test, y_test)]*len(valid_loss),  '--', color = 'blue', label = 'logreg test set acc')\n",
    "ax1.plot(range(len(valid_loss)), [net.score(X_test, y_test)]*len(valid_loss), '--', color = 'firebrick', label = 'neural net test set acc')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(valid_loss)), train_loss, '--', alpha = 0.5, color='firebrick', label = 'neural net train loss')\n",
    "ax2.plot(range(len(valid_loss)), valid_loss, '--', color='red', label = 'neural net validation loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax2.set_ylabel('train loss')\n",
    "ax1.grid(True)\n",
    "ax1.legend(loc = 3)\n",
    "ax2.legend(loc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
