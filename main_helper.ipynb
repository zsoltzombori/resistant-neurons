{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "# import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torchnet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "DATASET = \"fashion_mnist\"\n",
    "TRAINSIZE = 60000\n",
    "SEED = None\n",
    "DROPOUT = 0.25\n",
    "BATCH_SIZE = 500\n",
    "DEPTH = 5\n",
    "WIDTH = 100\n",
    "OUTPUT_COUNT = 10\n",
    "LR = 0.002\n",
    "L1REG = 0.01\n",
    "MEMORY_SHARE = 0.05\n",
    "ITERS = 30\n",
    "EVALUATION_CHECKPOINT = 1\n",
    "AUGMENTATION = False\n",
    "SESSION_NAME = \"sinusoidal_5_100_KP_{}_{}\".format(DROPOUT, time.strftime('%Y%m%d-%H%M%S'))\n",
    "BN_WEIGHT = 0\n",
    "COV_WEIGHT = 0\n",
    "CLASSIFIER_TYPE = \"dense\"  # \"conv\" / \"dense\"\n",
    "LOG_DIR = \"logs/%s\" % SESSION_NAME\n",
    "EVALUATE_USEFULNESS = True\n",
    "USEFULNESS_EVAL_SET_SIZE = 1000\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "train_dataset = FashionMNIST('.', train=True, download=True,\n",
    "                             transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset = FashionMNIST('.', train=False, download=True,\n",
    "                            transform=transforms.Compose([transforms.ToTensor()]))\n",
    "eval_dataset = torch.utils.data.Subset(test_dataset, range(USEFULNESS_EVAL_SET_SIZE))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)\n",
    "\n",
    "eval_loader = DataLoader(dataset=eval_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(net, device):\n",
    "    net.eval()\n",
    "    correct = 0.\n",
    "    total = len(test_dataset)\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # torch.argmax(net(images), dim=1) == labels\n",
    "        correct += torch.sum(torch.argmax(net(images), dim=1) == labels).cpu().numpy()\n",
    "\n",
    "    return(correct/total)\n",
    "\n",
    "\n",
    "def calculate_l1loss(net):\n",
    "    # l1loss = torch.autograd.Variable(torch.tensor(0, dtype=torch.float, requires_grad=True))\n",
    "    l1loss = 0.\n",
    "    for name, param in net.named_parameters():\n",
    "        if param.requires_grad and 'weight' in name:\n",
    "            l1loss += torch.mean(torch.abs(param))\n",
    "\n",
    "    return(l1loss * L1REG)\n",
    "\n",
    "\n",
    "def get_weights_for_position(pos, net, direction='input'):\n",
    "\n",
    "    d, p = pos\n",
    "    weight_layers = []\n",
    "    for name, weights in net.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            weight_layers += [weights.cpu().detach().numpy()]\n",
    "\n",
    "    return(weight_layers[d][p, :])\n",
    "\n",
    "\n",
    "def get_grad_for_position(pos, net, direction='input'):\n",
    "\n",
    "    d, p = pos\n",
    "    grads = []\n",
    "    for name, weights in net.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            grads += [weights.cpu().detach().numpy()]\n",
    "\n",
    "    return(grads[d][p, :])\n",
    "\n",
    "\n",
    "def zero_grad_for_neuron(pos, net, direction='input'):\n",
    "    d, p = pos\n",
    "    i = 0\n",
    "    for name, weights in net.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            if i == d:\n",
    "                # print(weights.grad[p, :].size())\n",
    "                weights.grad[p, :] = 0\n",
    "                return()\n",
    "            i += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanishingDataset(Dataset):\n",
    "    def __init__(self, list_of_image_target_tuples):\n",
    "        self.data = torch.stack([x[0] for x in list_of_image_target_tuples])\n",
    "        self.targets = torch.stack([x[1] for x in list_of_image_target_tuples])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return(self.data[idx], self.targets[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_add_topn_activations(net, layer, frozen_neurons, topn, hidden_activations):\n",
    "    \n",
    "    hidden_activations_for_layer = np.concatenate([x[layer] for x in hidden_activations], axis=0)\n",
    "    sum_activations_per_neuron = np.sum(np.abs(hidden_activations_for_layer), axis=0)\n",
    "    sorted_by_activation = np.argsort(sum_activations_per_neuron)[::-1]\n",
    "    #print(f'frozen_neurons: {frozen_neurons}')\n",
    "    old_frozen_neurons = [x[1] for x in frozen_neurons if x[0] == layer]\n",
    "    #print(f'old frozen: {old_frozen_neurons}')\n",
    "    new_frozen_neurons = []\n",
    "    #print(sorted_by_activation)\n",
    "    for n_i in sorted_by_activation:\n",
    "        if len(new_frozen_neurons) < topn:\n",
    "            # print(n_i)\n",
    "            if n_i in old_frozen_neurons:\n",
    "                pass\n",
    "                #print(f'{n_i} is already in frozen neurons')\n",
    "            else:\n",
    "                #print(f'{n_i} is not in frozen neurons, adding')\n",
    "                new_frozen_neurons += [n_i]\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    frozen_neurons += [(layer, n_i) for n_i in new_frozen_neurons]\n",
    "    return(frozen_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNet(\n",
      "  (fcin): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (do1): Dropout(p=0.25, inplace=False)\n",
      "  (relu1): ReLU()\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): Dropout(p=0.25, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (4): Dropout(p=0.25, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): Dropout(p=0.25, inplace=False)\n",
      "    (8): ReLU()\n",
      "  )\n",
      "  (fclass): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "number of bad guesses: 25107\n",
      "001/030 Train loss: 1.886    Train accuracy: 0.582    Test accuracy: 0.756\tEpoch time: 11.67    L1Loss: 0.002\tWeight sum: 4330.700\n",
      "number of bad guesses: 13962\n",
      "002/030 Train loss: 1.699    Train accuracy: 0.767    Test accuracy: 0.776\tEpoch time: 12.26    L1Loss: 0.003\tWeight sum: 4867.904\n",
      "number of bad guesses: 12872\n",
      "003/030 Train loss: 1.679    Train accuracy: 0.785    Test accuracy: 0.792\tEpoch time: 11.71    L1Loss: 0.003\tWeight sum: 5233.643\n",
      "number of bad guesses: 12337\n",
      "004/030 Train loss: 1.670    Train accuracy: 0.794    Test accuracy: 0.800\tEpoch time: 12.46    L1Loss: 0.003\tWeight sum: 5544.457\n",
      "number of bad guesses: 12136\n",
      "005/030 Train loss: 1.666    Train accuracy: 0.798    Test accuracy: 0.803\tEpoch time: 12.51    L1Loss: 0.003\tWeight sum: 5836.406\n",
      "number of bad guesses: 11328\n",
      "006/030 Train loss: 1.654    Train accuracy: 0.811    Test accuracy: 0.831\tEpoch time: 12.73    L1Loss: 0.003\tWeight sum: 6134.932\n",
      "number of bad guesses: 10641\n",
      "007/030 Train loss: 1.642    Train accuracy: 0.823    Test accuracy: 0.819\tEpoch time: 11.59    L1Loss: 0.003\tWeight sum: 6426.130\n",
      "number of bad guesses: 10550\n",
      "008/030 Train loss: 1.641    Train accuracy: 0.824    Test accuracy: 0.831\tEpoch time: 7.69    L1Loss: 0.004\tWeight sum: 6713.059\n",
      "number of bad guesses: 10257\n",
      "009/030 Train loss: 1.636    Train accuracy: 0.829    Test accuracy: 0.826\tEpoch time: 10.68    L1Loss: 0.004\tWeight sum: 6934.741\n",
      "number of bad guesses: 9859\n",
      "010/030 Train loss: 1.629    Train accuracy: 0.836    Test accuracy: 0.829\tEpoch time: 12.39    L1Loss: 0.004\tWeight sum: 7137.197\n",
      "number of bad guesses: 9796\n",
      "011/030 Train loss: 1.628    Train accuracy: 0.837    Test accuracy: 0.840\tEpoch time: 11.25    L1Loss: 0.004\tWeight sum: 7351.357\n",
      "number of bad guesses: 9570\n",
      "012/030 Train loss: 1.624    Train accuracy: 0.841    Test accuracy: 0.836\tEpoch time: 11.10    L1Loss: 0.004\tWeight sum: 7543.846\n",
      "number of bad guesses: 9624\n",
      "013/030 Train loss: 1.625    Train accuracy: 0.840    Test accuracy: 0.844\tEpoch time: 7.60    L1Loss: 0.004\tWeight sum: 7752.333\n",
      "number of bad guesses: 9368\n",
      "014/030 Train loss: 1.621    Train accuracy: 0.844    Test accuracy: 0.833\tEpoch time: 12.81    L1Loss: 0.004\tWeight sum: 7906.374\n",
      "number of bad guesses: 9594\n",
      "015/030 Train loss: 1.625    Train accuracy: 0.840    Test accuracy: 0.842\tEpoch time: 12.85    L1Loss: 0.004\tWeight sum: 8101.899\n",
      "number of bad guesses: 9558\n",
      "016/030 Train loss: 1.624    Train accuracy: 0.841    Test accuracy: 0.839\tEpoch time: 13.67    L1Loss: 0.004\tWeight sum: 8307.221\n",
      "length of the remaining images: 9558\n",
      "[(0, 12), (0, 11), (0, 55), (0, 23), (0, 67), (0, 27), (0, 76), (0, 33), (0, 34), (0, 73), (0, 31), (0, 16), (0, 78), (0, 57), (0, 50), (0, 54), (0, 6), (0, 36), (0, 84), (0, 53), (0, 48), (0, 14), (0, 45), (0, 79), (0, 9), (0, 44), (0, 59), (0, 58), (0, 40), (0, 61), (0, 22), (0, 15), (0, 69), (0, 85), (0, 19), (0, 35), (0, 52), (0, 90), (0, 4), (0, 39), (0, 60), (0, 99), (0, 95), (0, 0), (0, 88), (0, 68), (0, 24), (0, 30), (0, 64), (0, 91), (0, 47), (0, 5), (0, 41), (0, 37), (0, 63), (0, 8), (0, 10), (0, 89), (0, 93), (0, 66), (0, 77), (0, 38), (0, 72), (0, 32), (0, 46), (0, 49), (0, 65), (0, 75), (0, 86), (0, 96), (0, 56), (0, 51), (0, 7), (0, 82), (0, 18), (0, 1), (0, 97), (0, 80), (0, 25), (0, 94), (0, 26), (0, 17), (0, 74), (0, 42), (0, 98), (0, 70), (0, 20), (0, 62), (0, 3), (0, 28), (0, 81), (0, 2), (0, 92), (0, 13), (0, 43), (0, 21), (0, 29), (0, 71), (0, 83), (0, 87), (1, 7), (1, 63), (1, 39), (1, 60), (1, 32), (1, 49), (1, 38), (1, 97), (1, 51), (1, 94), (1, 1), (1, 77), (1, 24), (1, 93), (1, 10), (1, 22), (1, 56), (1, 14), (1, 17), (1, 26), (1, 37), (1, 96), (1, 75), (1, 95), (1, 19), (1, 74), (1, 47), (1, 78), (1, 86), (1, 31), (1, 30), (1, 66), (1, 48), (1, 45), (1, 18), (1, 58), (1, 36), (1, 84), (1, 46), (1, 91), (1, 40), (1, 0), (1, 65), (1, 79), (1, 44), (1, 99), (1, 54), (1, 57), (1, 82), (1, 5), (1, 23), (1, 34), (1, 8), (1, 25), (1, 73), (1, 16), (1, 89), (1, 50), (1, 53), (1, 52), (1, 15), (1, 9), (1, 85), (1, 68), (1, 72), (1, 59), (1, 4), (1, 69), (1, 64), (1, 80), (1, 67), (1, 55), (1, 76), (1, 12), (1, 35), (1, 11), (1, 41), (1, 61), (1, 6), (1, 27), (1, 90), (1, 88), (1, 21), (1, 42), (1, 92), (1, 2), (1, 3), (1, 98), (1, 62), (1, 43), (1, 70), (1, 20), (1, 71), (1, 33), (2, 69), (2, 42), (2, 23), (2, 43), (2, 13), (2, 41), (2, 48), (2, 70), (2, 89), (2, 51), (2, 27), (2, 44), (2, 81), (2, 36), (2, 91), (2, 78), (2, 55), (2, 77), (2, 0), (2, 17), (2, 95), (2, 22), (2, 34), (2, 57), (2, 5), (2, 15), (2, 93), (2, 47), (2, 46), (2, 31), (2, 33), (2, 80), (2, 20), (2, 49), (2, 10), (2, 52), (2, 4), (2, 66), (2, 62), (2, 40), (2, 11), (2, 56), (2, 3), (2, 39), (2, 58), (2, 63), (2, 87), (2, 26), (2, 83), (2, 82), (2, 60), (2, 18), (2, 29), (2, 35), (2, 9), (2, 61), (2, 85), (2, 45), (2, 54), (2, 92), (2, 67), (2, 88), (2, 76), (2, 28), (2, 25), (2, 65), (2, 24), (2, 16), (2, 86), (2, 19), (2, 75), (2, 79), (2, 99), (2, 30), (2, 59), (2, 2), (2, 50), (2, 38), (2, 68), (2, 37), (2, 94), (2, 14), (2, 1), (2, 84), (2, 21), (2, 72), (2, 12), (2, 32), (2, 74), (2, 73), (2, 53), (2, 98), (2, 71), (2, 64), (3, 46), (3, 21), (3, 17), (3, 99), (3, 22), (3, 75), (3, 9), (3, 33), (3, 84), (3, 68), (3, 41), (3, 61), (3, 7), (3, 73), (3, 98), (3, 83), (3, 18), (3, 28), (3, 2), (3, 32), (3, 27), (3, 97), (3, 0), (3, 38), (3, 6), (3, 16), (3, 40), (3, 62), (3, 86), (3, 77), (3, 71), (3, 53), (3, 36), (3, 96), (3, 37), (3, 31), (3, 69), (3, 82), (3, 89), (3, 44), (3, 70), (3, 24), (3, 57), (3, 90), (3, 3), (3, 79), (3, 30), (3, 49), (3, 29), (3, 43), (3, 92), (3, 13), (3, 4), (3, 59), (3, 25), (3, 52), (3, 72), (3, 64), (3, 81), (3, 10), (3, 93), (3, 76), (3, 58), (3, 80), (3, 95), (3, 88), (3, 87), (3, 60), (3, 51), (3, 78), (3, 39), (3, 45), (3, 20), (3, 11), (3, 67), (3, 14), (3, 50), (3, 94), (3, 1), (3, 85), (3, 47), (3, 26), (3, 48), (3, 63), (3, 74), (3, 54), (3, 35), (3, 5), (3, 19), (3, 56), (3, 65), (3, 23), (3, 42), (3, 91), (4, 5), (4, 6), (4, 7), (4, 8), (4, 0), (4, 1), (4, 2), (4, 9), (4, 4)]\n",
      "number of bad guesses: 6617\n",
      "017/030 Train loss: 0.359    Train accuracy: 0.308    Test accuracy: 0.800\tEpoch time: 2.17    L1Loss: 0.001\tWeight sum: 8342.344\n",
      "number of bad guesses: 6557\n",
      "018/030 Train loss: 0.358    Train accuracy: 0.314    Test accuracy: 0.796\tEpoch time: 2.89    L1Loss: 0.001\tWeight sum: 8351.742\n",
      "number of bad guesses: 6429\n",
      "019/030 Train loss: 0.356    Train accuracy: 0.327    Test accuracy: 0.794\tEpoch time: 2.61    L1Loss: 0.001\tWeight sum: 8357.228\n",
      "number of bad guesses: 6323\n",
      "020/030 Train loss: 0.355    Train accuracy: 0.338    Test accuracy: 0.792\tEpoch time: 2.66    L1Loss: 0.001\tWeight sum: 8365.729\n",
      "number of bad guesses: 6211\n",
      "021/030 Train loss: 0.351    Train accuracy: 0.350    Test accuracy: 0.776\tEpoch time: 2.45    L1Loss: 0.001\tWeight sum: 8378.095\n",
      "number of bad guesses: 5981\n",
      "022/030 Train loss: 0.347    Train accuracy: 0.374    Test accuracy: 0.771\tEpoch time: 2.56    L1Loss: 0.001\tWeight sum: 8390.115\n",
      "number of bad guesses: 5864\n",
      "023/030 Train loss: 0.346    Train accuracy: 0.386    Test accuracy: 0.751\tEpoch time: 2.18    L1Loss: 0.001\tWeight sum: 8399.706\n",
      "number of bad guesses: 5737\n",
      "024/030 Train loss: 0.344    Train accuracy: 0.400    Test accuracy: 0.735\tEpoch time: 2.10    L1Loss: 0.001\tWeight sum: 8408.319\n",
      "number of bad guesses: 5736\n",
      "025/030 Train loss: 0.344    Train accuracy: 0.400    Test accuracy: 0.728\tEpoch time: 2.31    L1Loss: 0.001\tWeight sum: 8414.738\n",
      "number of bad guesses: 5664\n",
      "026/030 Train loss: 0.343    Train accuracy: 0.407    Test accuracy: 0.725\tEpoch time: 2.48    L1Loss: 0.001\tWeight sum: 8419.428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bad guesses: 5646\n",
      "027/030 Train loss: 0.342    Train accuracy: 0.409    Test accuracy: 0.730\tEpoch time: 2.56    L1Loss: 0.001\tWeight sum: 8424.417\n",
      "number of bad guesses: 5583\n",
      "028/030 Train loss: 0.341    Train accuracy: 0.416    Test accuracy: 0.733\tEpoch time: 1.91    L1Loss: 0.001\tWeight sum: 8431.051\n",
      "number of bad guesses: 5582\n",
      "029/030 Train loss: 0.341    Train accuracy: 0.416    Test accuracy: 0.730\tEpoch time: 2.03    L1Loss: 0.001\tWeight sum: 8437.165\n",
      "number of bad guesses: 5556\n",
      "030/030 Train loss: 0.340    Train accuracy: 0.419    Test accuracy: 0.727\tEpoch time: 2.26    L1Loss: 0.001\tWeight sum: 8441.658\n",
      "Training took 257.15 seconds\n"
     ]
    }
   ],
   "source": [
    "net = torchnet.FFNet(WIDTH, DEPTH, DROPOUT, OUTPUT_COUNT)\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=0)\n",
    "\n",
    "minibatches = len(train_dataset) // BATCH_SIZE\n",
    "\n",
    "neurons_to_freeze = []\n",
    "vanish_dataloader = train_loader\n",
    "\n",
    "for epoch in range(ITERS):  # loop over the dataset multiple times\n",
    "    running_predictions = 0.\n",
    "    running_loss = 0.0\n",
    "    running_l1loss = 0.0\n",
    "    hidden_activations_for_epoch = []\n",
    "    epochtime = time.time()\n",
    "    net = net.train()\n",
    "    samples_seen = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    list_of_data = []    \n",
    "    \n",
    "    for i, data in enumerate(vanish_dataloader, 0):\n",
    "        \n",
    "        \n",
    "            \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(images)\n",
    "        \n",
    "        \n",
    "        predictions = torch.argmax(outputs, dim = 1)\n",
    "        for j in range(data[0].shape[0]):\n",
    "            im, tag, pred = data[0][j].squeeze(), data[1][j].squeeze(), predictions[j]\n",
    "            if tag != pred:\n",
    "                list_of_data += [tuple([im, tag])]\n",
    "        \n",
    "        \n",
    "            \n",
    "        hidden_activations_for_epoch += [net.hidden_activations]\n",
    "        # l1loss = calculate_l1loss(net)\n",
    "        l1loss = calculate_l1loss(net)\n",
    "        loss = criterion(outputs, labels) + l1loss\n",
    "\n",
    "        running_l1loss += l1loss\n",
    "        loss.backward()\n",
    "        # we have the gradients at this point, and they are encoded in param.grad where param is net.parameters()\n",
    "        if epoch >= 0:\n",
    "            for pos in neurons_to_freeze:\n",
    "                zero_grad_for_neuron(pos, net)\n",
    "\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss\n",
    "        running_predictions += torch.sum(torch.argmax(outputs, dim=1) == labels).cpu().numpy()\n",
    "        samples_seen += images.shape[0]\n",
    "        \n",
    "    #  = (0, 5)\n",
    "    # print(np.sum(np.abs(get_weights_for_position(pos, net))))\n",
    "    print(f'number of bad guesses: {len(list_of_data)}')\n",
    "    \n",
    "    print(f'{epoch + 1:03d}/{ITERS:03d} Train loss: {running_loss.cpu() / minibatches:.3f}\\\n",
    "    Train accuracy: {running_predictions/samples_seen:.3f}\\\n",
    "    Test accuracy: {test_epoch(net, device):.3f}\\tEpoch time: {time.time()-epochtime:.2f}\\\n",
    "    L1Loss: {running_l1loss.cpu() / minibatches:.3f}\\tWeight sum: {sum([p.abs().sum() for p in net.parameters()]):.3f}')\n",
    "    \n",
    "    layer=1\n",
    "    \n",
    "    ratio_to_freeze = 0.949\n",
    "    \n",
    "    if epoch == 15:\n",
    "        vanishing_dataset = VanishingDataset(list_of_data)\n",
    "        vanish_dataloader = DataLoader(dataset=vanishing_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        neurons_to_freeze = get_and_add_topn_activations(net, 0, neurons_to_freeze, int(28*28*ratio_to_freeze), hidden_activations_for_epoch)\n",
    "        neurons_to_freeze = get_and_add_topn_activations(net, DEPTH-1, neurons_to_freeze, int(OUTPUT_COUNT*ratio_to_freeze), hidden_activations_for_epoch)\n",
    "        for l in range(1, DEPTH-1):\n",
    "            topn = int(WIDTH * ratio_to_freeze)\n",
    "            neurons_to_freeze = get_and_add_topn_activations(net, l, neurons_to_freeze, topn, hidden_activations_for_epoch)\n",
    "            \n",
    "            \n",
    "        neurons_to_freeze = sorted(neurons_to_freeze, key = lambda x: x[0])\n",
    "        print(f'length of the remaining images: {len(vanishing_dataset)}')\n",
    "        print(neurons_to_freeze)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if False:\n",
    "        hidden_activations_for_layer = np.concatenate([x[layer] for x in hidden_activations_for_epoch], axis=0)\n",
    "        plt.plot(figsize = (12, 6), facecolor = 'w')\n",
    "        plt.hist(np.sum(np.abs(hidden_activations_for_layer), axis=0))\n",
    "        plt.title(f'sum of activations in epoch {epoch}')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "endtime = time.time()\n",
    "print(f'Training took {endtime-starttime:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34000, 28, 28]) torch.Size([34000])\n"
     ]
    }
   ],
   "source": [
    "ims = [x[0] for x in list_of_data]\n",
    "lbls = [x[1] for x in list_of_data]\n",
    "ims = torch.stack(ims)\n",
    "lbls = torch.stack(lbls)\n",
    "print(ims.shape, lbls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_index_sampler',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'dataset_kind',\n",
       " 'drop_last',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'pin_memory',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FashionMNIST in module torchvision.datasets.mnist:\n",
      "\n",
      "class FashionMNIST(MNIST)\n",
      " |  FashionMNIST(root, train=True, transform=None, target_transform=None, download=False)\n",
      " |  \n",
      " |  `Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n",
      " |  \n",
      " |  Args:\n",
      " |      root (string): Root directory of dataset where ``Fashion-MNIST/processed/training.pt``\n",
      " |          and  ``Fashion-MNIST/processed/test.pt`` exist.\n",
      " |      train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      " |          otherwise from ``test.pt``.\n",
      " |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      " |          puts it in root directory. If dataset is already downloaded, it is not\n",
      " |          downloaded again.\n",
      " |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      " |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      " |      target_transform (callable, optional): A function/transform that takes in the\n",
      " |          target and transforms it.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FashionMNIST\n",
      " |      MNIST\n",
      " |      torchvision.datasets.vision.VisionDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'San...\n",
      " |  \n",
      " |  urls = ['http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/tr...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from MNIST:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Args:\n",
      " |          index (int): Index\n",
      " |      \n",
      " |      Returns:\n",
      " |          tuple: (image, target) where target is index of the target class.\n",
      " |  \n",
      " |  __init__(self, root, train=True, transform=None, target_transform=None, download=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  download(self)\n",
      " |      Download the MNIST data if it doesn't exist in processed_folder already.\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from MNIST:\n",
      " |  \n",
      " |  class_to_idx\n",
      " |  \n",
      " |  processed_folder\n",
      " |  \n",
      " |  raw_folder\n",
      " |  \n",
      " |  test_data\n",
      " |  \n",
      " |  test_labels\n",
      " |  \n",
      " |  train_data\n",
      " |  \n",
      " |  train_labels\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from MNIST:\n",
      " |  \n",
      " |  test_file = 'test.pt'\n",
      " |  \n",
      " |  training_file = 'training.pt'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(FashionMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
